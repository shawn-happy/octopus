sources:
  - type: csv                                     # SourceType: [csv, parquet, json, orc, jdbc, iceberg, hive]
    name: user                                    # name: unique source name
    output: user                                  # output: unique output
    repartition: 2                                # repartition
    options: # CSVSource Options
      pathGlobFilter:                             # e.g: *.parquet, default null
      recursiveFileLookup: true                   # recursive look up file in dictionary
      paths: [ "src/test/resources/user.csv" ]                              # file paths, array type
      schemas: # source schema
        - name: id
          alias: user_id
          type: Integer
          nullable: false
          primaryKey: true
        - name: name
          alias: username
          type: String
          nullable: false
          primaryKey: false
        - name: createTime
          alias: create_time
          type: Date
          nullable: true
          primaryKey: false
#      header: true                                 # first line of csv file is header?. default true
#      encoding: UTF-8                              # csv file encoding. default utf-8
#      nullValue: null                              # replace null value. default null
#      nanValue: NaN                                # replace nan value. default NaN
#      dateFormat: yyyy-MM-dd                       # date format. default yyyy-MM-dd. e.g 2020-05-22
#      dateTimeFormat: yyyy-MM-dd HH:mm:ss.SSS      # dateTime format. default yyyy-MM-dd HH:mm:ss.SSS. e.g 2020-05-22 13:44:23.000
#      parseErrorPolicy: PERMISSIVE                 # parse error policy.   [PERMISSIVE,DROPMALFORMED, FAILFAST]
#      inferSchema: false                           # auto infer schema. default false

transforms:
  - type: sparkSQL
    name: user-trans
    input:
      user: user_temp
    output: user_trans_output
    repartition: 2
    options:
      sql: select *, (FLOOR(RAND() * 100) - 10) as age from user_temp

sinks:
  - type: csv                                         # sink type: [parquet, csv, json, jdbc, iceberg, hive]
    name: user-sink                                   # unique sink name
    input: user_trans_output                                 # dataframe name
    writeMode: append                                 # write mode: [append, replace, create_or_replace, replace_by_time, overwrite_partitions]
    options:
      path: "output/user-sink.csv"                                    # sink path
#      hasHeader: false                                # first line is header. default false
#      encoding: "UTF-8"                               # default utf-8
#      dateFormat: "yyyy-MM-dd"                        # default yyyy-MM-dd
#      dateTimeFormat: "yyyy-MM-dd HH:mm:ss.SSS"       # default yyyy-MM-dd HH:mm:ss.SSS

sparkConf:
  spark.driver.cores: 1
  spark.driver.memory: 1g
