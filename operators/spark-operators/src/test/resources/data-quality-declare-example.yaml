sources:
  - type: csv                                     # SourceType: [csv, parquet, json, orc, jdbc, iceberg, hive]
    name: user                                    # name: unique source name
    output: user                                  # output: unique output
    repartition: 2                                # repartition
    options: # CSVSource Options
      pathGlobFilter:                             # e.g: *.parquet, default null
      recursiveFileLookup: true                   # recursive look up file in dictionary
      paths: [ "src/test/resources/user-report.csv" ]                              # file paths, array type
      schemas: # source schema
        - name: id
          alias: user_id
          type: Integer
          nullable: false
          primaryKey: true
        - name: name
          alias: username
          type: String
          nullable: false
          primaryKey: false
        - name: createTime
          alias: create_time
          type: Date
          nullable: true
          primaryKey: false
        - name: age
          alias:
          type: Integer
          nullable: true
          primaryKey: false
      header: true                                 # first line of csv file is header?. default true
#      encoding: UTF-8                              # csv file encoding. default utf-8
#      nullValue: null                              # replace null value. default null
#      nanValue: NaN                                # replace nan value. default NaN
#      dateFormat: yyyy-MM-dd                       # date format. default yyyy-MM-dd. e.g 2020-05-22
#      dateTimeFormat: yyyy-MM-dd HH:mm:ss.SSS      # dateTime format. default yyyy-MM-dd HH:mm:ss.SSS. e.g 2020-05-22 13:44:23.000
#      parseErrorPolicy: PERMISSIVE                 # parse error policy.   [PERMISSIVE,DROPMALFORMED, FAILFAST]
#      inferSchema: false                           # auto infer schema. default false

metrics:
  - type: metrics
    name: user-metrics
    input:
      user: user_temp
    output: user-age-avg
    options:
      columns: [ "age" ]
      opType: mean
  - type: sparkSQL
    name: user-age-lt-zero-count
    input:
      user: user_temp
    output: age_lt_0_count
    options:
      sql: select count(*) as age_lt_0_count from user_temp where age < 0;

checks:
  - type: expression
    name: user-age-check
    metrics: [ "age_lt_0_count" ]
    output: age-lt-0-count-check
    repartition: 2
    checkLevel: warning
    options:
      expression: "age_lt_0_count<=0"

postProcesses:
  - type: correction
    name: user-age-correction
    options:
      mode: modify  #[replace, modify]
      source: user
      sql: select * from user where age >= 0

filePath: output/user-avg-dq-result.json