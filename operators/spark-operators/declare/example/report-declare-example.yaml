sources:
  - type: csv                                     # SourceType: [csv, parquet, json, orc, jdbc, iceberg, hive]
    name: csv-source-example                      # name: unique source name
    output: csv-source-output                     # output: unique output
    repartition: 2                                # repartition
    options: # CSVSource Options
      pathGlobFilter:                             # e.g: *.parquet, default null
      recursiveFileLookup: true                   # recursive look up file in dictionary
      paths: [ '','' ]                              # file paths, array type
      schemas: # source schema
        - name: id
          alias: user_id
          type: Integer
          nullable: false
          primaryKey: true
        - name: name
          alias: username
          type: String
          nullable: false
          primaryKey: false
        - name: updateTime
          alias: update_time
          type: Date
          nullable: true
          primaryKey: false
      header: true                                 # first line of csv file is header?. default true
      encoding: UTF-8                              # csv file encoding. default utf-8
      nullValue: null                              # replace null value. default null
      nanValue: NaN                                # replace nan value. default NaN
      dateFormat: yyyy-MM-dd                       # date format. default yyyy-MM-dd. e.g 2020-05-22
      dateTimeFormat: yyyy-MM-dd HH:mm:ss.SSS      # dateTime format. default yyyy-MM-dd HH:mm:ss.SSS. e.g 2020-05-22 13:44:23.000
      parseErrorPolicy: PERMISSIVE                 # parse error policy.   [PERMISSIVE,DROPMALFORMED, FAILFAST]
      inferSchema: false                           # auto infer schema. default false

metircs:
  - type: metrics
    name: metrics-trans
    input:
      input_name: input_name_alias
    output: metrics_trans_output
    repartition: 2
    options:
      opType: count # [count,distinctCount,nullCount,nullOrEmptyCount,min,max,mean,variance,stddev,median,distinct,distribution,approxMedian,approxCountDistinct,uniqueRatio,nullRatio,joinRatio,storageSize,approxQuantiles,summary,psi]
      columns: [ "col1" ]

sinks:
  - type: csv                                         # sink type: [parquet, csv, json, jdbc, iceberg, hive]
    name: csv-sink                                    # unique sink name
    input: csv-input                                  # dataframe name
    writeMode: append                                 # write mode: [append, replace, create_or_replace, replace_by_time, overwrite_partitions]
    options:
      path: /path                                     # sink path
      hasHeader: false                                # first line is header. default false
      encoding: "UTF-8"                               # default utf-8
      dateFormat: "yyyy-MM-dd"                        # default yyyy-MM-dd
      dateTimeFormat: "yyyy-MM-dd HH:mm:ss.SSS"       # default yyyy-MM-dd HH:mm:ss.SSS